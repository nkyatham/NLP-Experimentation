{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-06-15T22:01:04.340628Z","iopub.execute_input":"2022-06-15T22:01:04.341203Z","iopub.status.idle":"2022-06-15T22:01:05.221211Z","shell.execute_reply.started":"2022-06-15T22:01:04.341090Z","shell.execute_reply":"2022-06-15T22:01:05.219988Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Wed Jun 15 22:01:05 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# import tensorflow as tf\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# with tpu_strategy.scope():\n#     model = tf.keras.Sequential( … ) # define your model normally\n#     model.compile( … )\n    \n# model.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=…)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/abstract'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        print(dirname)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-16T10:44:05.021985Z","iopub.execute_input":"2022-06-16T10:44:05.022474Z","iopub.status.idle":"2022-06-16T10:44:05.066979Z","shell.execute_reply.started":"2022-06-16T10:44:05.022355Z","shell.execute_reply":"2022-06-16T10:44:05.066122Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/abstract/sample_submission.csv\n/kaggle/input/abstract\n/kaggle/input/abstract/train.csv\n/kaggle/input/abstract\n/kaggle/input/abstract/test.csv\n/kaggle/input/abstract\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrainData = pd.read_csv('/kaggle/input/abstract/train.csv', sep=',', header=\"infer\")\ntestData = pd.read_csv('/kaggle/input/abstract/test.csv', sep = ',', header = \"infer\")\ntestLabels = pd.read_csv('/kaggle/input/abstract/sample_submission.csv', sep = ',', header = \"infer\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:44:10.659419Z","iopub.execute_input":"2022-06-16T10:44:10.660076Z","iopub.status.idle":"2022-06-16T10:44:12.060690Z","shell.execute_reply.started":"2022-06-16T10:44:10.660041Z","shell.execute_reply":"2022-06-16T10:44:12.059511Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_data, remaining = train_test_split(trainData, train_size=0.85, random_state=34)\ntest_data, val_data = train_test_split(remaining, train_size=0.7, random_state=34)\ntrain_data.shape, val_data.shape, test_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:44:12.062271Z","iopub.execute_input":"2022-06-16T10:44:12.062925Z","iopub.status.idle":"2022-06-16T10:44:12.086323Z","shell.execute_reply.started":"2022-06-16T10:44:12.062876Z","shell.execute_reply":"2022-06-16T10:44:12.085184Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((18874, 9), (2098, 9))"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dataset Exploration","metadata":{}},{"cell_type":"code","source":"trainData.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T09:52:20.724678Z","iopub.execute_input":"2022-06-14T09:52:20.725178Z","iopub.status.idle":"2022-06-14T09:52:20.765445Z","shell.execute_reply.started":"2022-06-14T09:52:20.725141Z","shell.execute_reply":"2022-06-14T09:52:20.764397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainData[trainData['Computer Science'] == 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainData[trainData['Physics'] == 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainData[trainData['Mathematics'] == 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainData[trainData['Statistics'] == 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainData[trainData['Quantitative Biology'] == 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainData[trainData['Quantitative Finance'] == 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(trainData.groupby(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']).aggregate('Quantitative Finance'))","metadata":{"execution":{"iopub.status.busy":"2022-06-14T16:54:09.714613Z","iopub.execute_input":"2022-06-14T16:54:09.714959Z","iopub.status.idle":"2022-06-14T16:54:09.802809Z","shell.execute_reply.started":"2022-06-14T16:54:09.714919Z","shell.execute_reply":"2022-06-14T16:54:09.801795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nnp.where(trainData.iloc[:,3:].values == [0,0,1,1,0,1])","metadata":{"execution":{"iopub.status.busy":"2022-06-14T16:51:11.938017Z","iopub.execute_input":"2022-06-14T16:51:11.938934Z","iopub.status.idle":"2022-06-14T16:51:11.945592Z","shell.execute_reply.started":"2022-06-14T16:51:11.938889Z","shell.execute_reply":"2022-06-14T16:51:11.944579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData.columns.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS","metadata":{"execution":{"iopub.status.busy":"2022-06-14T16:32:23.354894Z","iopub.execute_input":"2022-06-14T16:32:23.355903Z","iopub.status.idle":"2022-06-14T16:32:23.461546Z","shell.execute_reply.started":"2022-06-14T16:32:23.355866Z","shell.execute_reply":"2022-06-14T16:32:23.460553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# -> Plot of class distributions","metadata":{}},{"cell_type":"code","source":"categories = list(trainData.columns.values[3:])\nsns.set(font_scale = 1)\nplt.figure(figsize=(15,8))\nax= sns.barplot(x = categories, y = trainData.iloc[:,3:].sum().values)\nplt.title(\"Abstract of each category\", fontsize=24)\nplt.ylabel('Number of abstracts', fontsize=18)\nplt.xlabel('Abstract Type ', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = trainData.iloc[:,3:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n    \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T16:32:24.770326Z","iopub.execute_input":"2022-06-14T16:32:24.771037Z","iopub.status.idle":"2022-06-14T16:32:25.071902Z","shell.execute_reply.started":"2022-06-14T16:32:24.770999Z","shell.execute_reply":"2022-06-14T16:32:25.070522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale = 1)\nplt.figure(figsize=(15,8))\nmultiLabel_counts = trainData.iloc[:,3:].sum(axis=1).value_counts()\n\nax = sns.barplot(x = multiLabel_counts.index,y = multiLabel_counts)\n\nplt.title(\"Abstracts with multiple labels \")\nplt.ylabel('Number of Abstracts', fontsize=18)\nplt.xlabel('Number of labels', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T16:32:31.898382Z","iopub.execute_input":"2022-06-14T16:32:31.898736Z","iopub.status.idle":"2022-06-14T16:32:32.127982Z","shell.execute_reply.started":"2022-06-14T16:32:31.898708Z","shell.execute_reply":"2022-06-14T16:32:32.126933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'./model/+{model_name}'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Model Implementation From Hugging Face","metadata":{}},{"cell_type":"code","source":"!pip install focal_loss\nfrom focal_loss import BinaryFocalLoss\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, DistilBertTokenizerFast\nimport pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport pickle\n\nprint(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n\n\nclass DistilBertTrain:\n    def __init__(self):\n        self.trainData = pd.read_csv('/kaggle/input/abstract/train.csv', sep=',', header=\"infer\")\n        self.trainData_cp = self.trainData.copy()\n        \n#         self.testData = pd.read_csv('/kaggle/input/abstract/test.csv', sep = ',', header = \"infer\")\n#         self.testLabels = pd.read_csv('/kaggle/input/abstract/sample_submission.csv', sep = ',', header = \"infer\")\n        self.MODEL_NAME = 'distilbert-base-uncased'\n        self.X = 'ABSTRACT'\n        self.Y = [3,4,5,6,7,8]\n        self.num_classes = len(self.Y)\n        self.BATCH_SIZE = 16\n        self.MAX_LENGTH = 512\n        self.N_EPOCHS = 5\n        self.lr=1e-5\n        self.tokenizer = DistilBertTokenizerFast.from_pretrained(self.MODEL_NAME)\n        self.model = TFDistilBertForSequenceClassification.from_pretrained(self.MODEL_NAME, num_labels = self.num_classes)\n        \n    def preprocess(self):\n        self.trainData_cp[self.X] = self.trainData[self.X].apply(lambda x: re.sub(\"\\n\",\" \",x))\n        train_data, val_data = train_test_split(self.trainData_cp, train_size=0.85, random_state=34)\n        val_data, test_data = train_test_split(val_data, train_size=0.7, random_state=34)\n        return train_data, val_data, test_data\n        \n    def distilBertTokenization(self, train_data, val_data):\n        train_encodings = self.tokenizer(train_data.ABSTRACT.to_list(), truncation=True, padding=True)\n        val_encodings = self.tokenizer(val_data.ABSTRACT.to_list(), truncation=True, padding=True)\n        return train_encodings, val_encodings\n    \n    def distilBertPipelineGeneration(self, train_encodings, val_encodings, train_data, val_data):\n        train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n                                    list(train_data.iloc[:,3:9].values)))\n        val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings),\n                                    list(val_data.iloc[:,3:9].values)))\n        # train\n        tr_pipe = (train_dataset.shuffle(len(train_data.ABSTRACT))\n                  .batch(self.BATCH_SIZE, drop_remainder=True)\n                    .prefetch(tf.data.experimental.AUTOTUNE)\n                      )\n\n  # valid\n        val_pipe = (val_dataset.batch(self.BATCH_SIZE, drop_remainder=True)\n                  .prefetch(tf.data.experimental.AUTOTUNE)\n                    )\n    \n        return tr_pipe, val_pipe\n    \n    def fit(self, tr_data, vl_data):\n        def scheduler(epoch, lr):\n            if epoch < 2:\n                return lr\n            else:\n                return lr * tf.math.exp(-0.1)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate= self.lr)\n\n        earlystp = tf.keras.callbacks.EarlyStopping(\n                    monitor='val_loss',\n                    min_delta=0,\n                    patience=3,\n                    verbose=1,\n                    mode='auto',\n                    baseline=None,\n                    restore_best_weights=True)\n\n\n\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n        loss = BinaryFocalLoss(gamma=2)\n\n        self.model.compile(optimizer, loss, metrics=['accuracy'])\n        self.model.fit(tr_data, epochs=self.N_EPOCHS, batch_size=self.BATCH_SIZE, validation_data = vl_data, callbacks =[lr_schedule, earlystp], verbose=1)\n        return self.model\n    \n    def save(self):\n        model_name = 'distilbert_base_uncased_model'\n        self.model.save_pretrained('./model/+{model_name}+')\n        with open('./model/info.pkl', 'wb') as f:\n            pickle.dump(('distilbert_base_uncased_model', self.MAX_LENGTH), f)\n            \n#     def load(self):\n#         new_model = TFDistilBertForSequenceClassification.from_pretrained('./model/distilbert_base_uncased_model')\n#         self.model_name, self.MAX_LENGTH = pickle.load(open('./model/info.pkl', 'rb'))\n            \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:09:46.970886Z","iopub.execute_input":"2022-06-16T22:09:46.971296Z","iopub.status.idle":"2022-06-16T22:10:23.260371Z","shell.execute_reply.started":"2022-06-16T22:09:46.971215Z","shell.execute_reply":"2022-06-16T22:10:23.259126Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting focal_loss\n  Downloading focal_loss-0.0.7-py3-none-any.whl (19 kB)\nRequirement already satisfied: tensorflow>=2.2 in /opt/conda/lib/python3.7/site-packages (from focal_loss) (2.6.4)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (3.20.1)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.12)\nCollecting wrapt~=1.12.1\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting absl-py~=0.10\n  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting six~=1.15.0\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (2.6.0)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (2.6.0)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.6.3)\nRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (2.6.0)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.4.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.1.0)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (5.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.2.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (0.37.1)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.43.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (1.1.2)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (3.3.0)\nRequirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.2->focal_loss) (3.1.0)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.2->focal_loss) (1.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (2.27.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (0.6.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (2.1.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (3.3.7)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (59.8.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (1.35.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (1.8.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (0.4.6)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (4.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (4.11.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (2022.5.18.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (2.0.12)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.2->focal_loss) (3.2.0)\nBuilding wheels for collected packages: wrapt\n  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=77037 sha256=1459db870ab610294aa21e4eef24a01efb1ffaa7df3214039984ba7e35cef87a\n  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\nSuccessfully built wrapt\nInstalling collected packages: wrapt, typing-extensions, six, numpy, absl-py, focal_loss\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.14.1\n    Uninstalling wrapt-1.14.1:\n      Successfully uninstalled wrapt-1.14.1\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.2.0\n    Uninstalling typing_extensions-4.2.0:\n      Successfully uninstalled typing_extensions-4.2.0\n  Attempting uninstall: six\n    Found existing installation: six 1.16.0\n    Uninstalling six-1.16.0:\n      Successfully uninstalled six-1.16.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: absl-py\n    Found existing installation: absl-py 1.0.0\n    Uninstalling absl-py-1.0.0:\n      Successfully uninstalled absl-py-1.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.8.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.8.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<2.9,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.8.0 requires tensorflow<3,>=2.8.0, but you have tensorflow 2.6.4 which is incompatible.\nrich 12.4.4 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.6.3 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.9 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.2 which is incompatible.\ngrpcio-status 1.46.1 requires grpcio>=1.46.1, but you have grpcio 1.43.0 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-cloud-storage<2.0.0dev,>=1.26.0, but you have google-cloud-storage 2.1.0 which is incompatible.\ngcsfs 2022.3.0 requires fsspec==2022.3.0, but you have fsspec 2022.5.0 which is incompatible.\nflax 0.5.0 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.11.4 which is incompatible.\nfeaturetools 1.9.0 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\napache-beam 2.38.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\napache-beam 2.38.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.4 which is incompatible.\naioitertools 0.10.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.3.2 requires botocore<1.24.22,>=1.24.21, but you have botocore 1.26.7 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed absl-py-0.15.0 focal_loss-0.0.7 numpy-1.19.5 six-1.15.0 typing-extensions-3.10.0.2 wrapt-1.12.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mTF Version:  2.6.4\nEager mode:  True\nGPU is available\n","output_type":"stream"},{"name":"stderr","text":"2022-06-16 22:10:23.250951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-06-16 22:10:23.252028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-06-16 22:10:23.252742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Class instance creation and invokation","metadata":{}},{"cell_type":"code","source":"model = DistilBertTrain()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:10:57.921911Z","iopub.execute_input":"2022-06-16T22:10:57.922821Z","iopub.status.idle":"2022-06-16T22:11:22.079608Z","shell.execute_reply.started":"2022-06-16T22:10:57.922785Z","shell.execute_reply":"2022-06-16T22:11:22.078824Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2d8a82a5ed84b4ebd1e8cd1f0519781"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01bd25f5eaf4d178b6a7d114f126af0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71081ac0aa5b41568c54197c01ea02cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9650f439e5d14f2b90420d929e844f3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d762bd5741b843839e81ee3802173950"}},"metadata":{}},{"name":"stderr","text":"2022-06-16 22:11:14.440634: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-06-16 22:11:14.441123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-06-16 22:11:14.442347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-06-16 22:11:14.443326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-06-16 22:11:19.692770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-06-16 22:11:19.693879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-06-16 22:11:19.694987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-06-16 22:11:19.695940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15047 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n2022-06-16 22:11:20.137451: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\nSome layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"train_data, val_data, test_data = model.preprocess()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:11:22.084119Z","iopub.execute_input":"2022-06-16T22:11:22.086231Z","iopub.status.idle":"2022-06-16T22:11:22.228329Z","shell.execute_reply.started":"2022-06-16T22:11:22.086191Z","shell.execute_reply":"2022-06-16T22:11:22.227412Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# DistilBERT Tokenization","metadata":{}},{"cell_type":"code","source":"train_encodings, val_encodings = model.distilBertTokenization(train_data, val_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:11:22.229602Z","iopub.execute_input":"2022-06-16T22:11:22.230071Z","iopub.status.idle":"2022-06-16T22:11:36.081032Z","shell.execute_reply.started":"2022-06-16T22:11:22.230029Z","shell.execute_reply":"2022-06-16T22:11:36.080209Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Tenforflow IO Pipeline creation using tf.data ","metadata":{}},{"cell_type":"code","source":"tr_pipe, val_pipe = model.distilBertPipelineGeneration(train_encodings, val_encodings, train_data, val_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:11:36.082908Z","iopub.execute_input":"2022-06-16T22:11:36.083278Z","iopub.status.idle":"2022-06-16T22:12:36.464672Z","shell.execute_reply.started":"2022-06-16T22:11:36.083242Z","shell.execute_reply":"2022-06-16T22:12:36.463789Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"distilBert=model.fit(tr_pipe, val_pipe)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:12:36.466111Z","iopub.execute_input":"2022-06-16T22:12:36.466481Z","iopub.status.idle":"2022-06-16T23:03:39.019308Z","shell.execute_reply.started":"2022-06-16T22:12:36.466445Z","shell.execute_reply":"2022-06-16T23:03:39.018422Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"2022-06-16 22:12:45.909910: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"1114/1114 [==============================] - 613s 541ms/step - loss: 0.0868 - accuracy: 0.6978 - val_loss: 0.0496 - val_accuracy: 0.7614\nEpoch 2/5\n1114/1114 [==============================] - 602s 541ms/step - loss: 0.0500 - accuracy: 0.7637 - val_loss: 0.0478 - val_accuracy: 0.7632\nEpoch 3/5\n1114/1114 [==============================] - 603s 541ms/step - loss: 0.0441 - accuracy: 0.7751 - val_loss: 0.0539 - val_accuracy: 0.7450\nEpoch 4/5\n1114/1114 [==============================] - 602s 541ms/step - loss: 0.0406 - accuracy: 0.7904 - val_loss: 0.0479 - val_accuracy: 0.7340\nEpoch 5/5\n1114/1114 [==============================] - 603s 541ms/step - loss: 0.0357 - accuracy: 0.7962 - val_loss: 0.0643 - val_accuracy: 0.7774\nRestoring model weights from the end of the best epoch.\nEpoch 00005: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_name = 'distilbert_base_uncased_model'\ndistilBert.save_pretrained('./model/distilbert_base_uncased_model')\nwith open('./model/info.pkl', 'wb') as f:\n    pickle.dump(('distilbert_base_uncased_model', 512), f)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:07:47.715407Z","iopub.execute_input":"2022-06-16T23:07:47.715861Z","iopub.status.idle":"2022-06-16T23:07:48.342252Z","shell.execute_reply.started":"2022-06-16T23:07:47.715824Z","shell.execute_reply":"2022-06-16T23:07:48.341307Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Model Prediction","metadata":{}},{"cell_type":"code","source":"# class DistilBertPredict:\n#     def __init__(self, model, test_data):\n#         self.\n    \n    \n#     def predict(self, test_data):","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DistilBertConfig()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T11:19:06.330527Z","iopub.execute_input":"2022-06-16T11:19:06.330914Z","iopub.status.idle":"2022-06-16T11:19:06.338123Z","shell.execute_reply.started":"2022-06-16T11:19:06.330882Z","shell.execute_reply":"2022-06-16T11:19:06.337280Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"transformers_version\": \"4.18.0\",\n  \"vocab_size\": 30522\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Dynamic sentence splitting and check for the confidence on each consecutive addition to last sentence","metadata":{}},{"cell_type":"code","source":"import spacy\n# ! python -m spacy download en_core_web_lg\n# spacy_lg = spacy.load('en_core_web_lg')\n! python -m spacy download en_core_web_sm\nspacy_sm = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:50:08.010841Z","iopub.execute_input":"2022-06-16T10:50:08.011301Z","iopub.status.idle":"2022-06-16T10:50:27.767099Z","shell.execute_reply.started":"2022-06-16T10:50:08.011265Z","shell.execute_reply":"2022-06-16T10:50:27.765866Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.2.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-sm==3.2.0) (3.2.4)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (59.8.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\nRequirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.16)\nRequirement already satisfied: click<8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.7)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\nRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\nRequirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.11.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.5.18.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef dynamicSentenceSelection(text, model):\n    sent_list = list(spacy_sm(train_data.ABSTRACT[0]).sents)\n    num_sents = len(sent_list)\n    prob_list = np.zeros([num_sents])\n    \n    for i in np.arange(num_sents):\n        np.append(prob_list, model.predict(sent_list[:i]))\n        \n    np.mean(prob_list)\n    return prob_list\n        \ndynamicSentenceSelection(train_data.ABSTRACT[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:51:32.573019Z","iopub.execute_input":"2022-06-16T10:51:32.573428Z","iopub.status.idle":"2022-06-16T10:51:32.708001Z","shell.execute_reply.started":"2022-06-16T10:51:32.573383Z","shell.execute_reply":"2022-06-16T10:51:32.707222Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"  Predictive models allow subject-specific inference when analyzing disease\nrelated alterations in neuroimaging data.\nGiven a subject's data, inference can\nbe made at two levels: global, i.e. identifiying condition presence for the\nsubject, and local, i.e. detecting condition effect on each individual\nmeasurement extracted from the subject's data.\nWhile global inference is widely\nused, local inference, which can be used to form subject-specific effect maps,\nis rarely used because existing models often yield noisy detections composed of\ndispersed isolated islands.\nIn this article, we propose a reconstruction\nmethod, named RSM, to improve subject-specific detections of predictive\nmodeling approaches and in particular, binary classifiers.\nRSM specifically\naims to reduce noise due to sampling error associated with using a finite\nsample of examples to train classifiers.\nThe proposed method is a wrapper-type\nalgorithm that can be used with different binary classifiers in a diagnostic\nmanner, i.e. without information on condition presence.\nReconstruction is posed\nas a Maximum-A-Posteriori problem with a prior model whose parameters are\nestimated from training data in a classifier-specific fashion.\nExperimental\nevaluation is performed on synthetically generated data and data from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database.\nResults on\nsynthetic data demonstrate that using RSM yields higher detection accuracy\ncompared to using models directly or with bootstrap averaging.\nAnalyses on the\nADNI dataset show that RSM can also improve correlation between\nsubject-specific detections in cortical thickness data and non-imaging markers\nof Alzheimer's Disease (AD), such as the Mini Mental State Examination Score\nand Cerebrospinal Fluid amyloid-$\\beta$ levels.\nFurther reliability studies on\nthe longitudinal ADNI dataset show improvement on detection reliability when\nRSM is used.\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"Hyperparameter Tuning\n\n1. Learning rate\n2. Early stopping\n3. Gradient clipping\n4. number of epochs to train\n5. dropout\n6. regularization","metadata":{},"execution_count":null,"outputs":[]}]}