{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-06-15T22:01:04.340628Z","iopub.execute_input":"2022-06-15T22:01:04.341203Z","iopub.status.idle":"2022-06-15T22:01:05.221211Z","shell.execute_reply.started":"2022-06-15T22:01:04.34109Z","shell.execute_reply":"2022-06-15T22:01:05.219988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/abstract'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        print(dirname)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-16T23:13:47.752178Z","iopub.execute_input":"2022-06-16T23:13:47.752545Z","iopub.status.idle":"2022-06-16T23:13:47.78009Z","shell.execute_reply.started":"2022-06-16T23:13:47.752514Z","shell.execute_reply":"2022-06-16T23:13:47.779237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrainData = pd.read_csv('/kaggle/input/abstract/train.csv', sep=',', header=\"infer\")\n# testData = pd.read_csv('/kaggle/input/abstract/test.csv', sep = ',', header = \"infer\")\n# testLabels = pd.read_csv('/kaggle/input/abstract/sample_submission.csv', sep = ',', header = \"infer\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:48.940171Z","iopub.execute_input":"2022-06-16T23:13:48.940548Z","iopub.status.idle":"2022-06-16T23:13:49.377509Z","shell.execute_reply.started":"2022-06-16T23:13:48.940518Z","shell.execute_reply":"2022-06-16T23:13:49.376731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, remaining = train_test_split(trainData, train_size=0.85, random_state=34)\ntest_data, val_data = train_test_split(remaining, train_size=0.7, random_state=34)\ntrain_data.shape, val_data.shape, test_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:51.072184Z","iopub.execute_input":"2022-06-16T23:13:51.072603Z","iopub.status.idle":"2022-06-16T23:13:51.088922Z","shell.execute_reply.started":"2022-06-16T23:13:51.072562Z","shell.execute_reply":"2022-06-16T23:13:51.087541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Exploration","metadata":{}},{"cell_type":"code","source":"trainData.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:54.198518Z","iopub.execute_input":"2022-06-16T23:13:54.19902Z","iopub.status.idle":"2022-06-16T23:13:54.219103Z","shell.execute_reply.started":"2022-06-16T23:13:54.198988Z","shell.execute_reply":"2022-06-16T23:13:54.218424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:55.358278Z","iopub.execute_input":"2022-06-16T23:13:55.358933Z","iopub.status.idle":"2022-06-16T23:13:55.397934Z","shell.execute_reply.started":"2022-06-16T23:13:55.358896Z","shell.execute_reply":"2022-06-16T23:13:55.397193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainData[trainData['Computer Science'] == 1])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:56.573937Z","iopub.execute_input":"2022-06-16T23:13:56.574285Z","iopub.status.idle":"2022-06-16T23:13:56.582204Z","shell.execute_reply.started":"2022-06-16T23:13:56.574256Z","shell.execute_reply":"2022-06-16T23:13:56.581345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(trainData.groupby(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']).aggregate('Quantitative Finance'))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:14:12.612477Z","iopub.execute_input":"2022-06-16T23:14:12.613072Z","iopub.status.idle":"2022-06-16T23:14:12.726468Z","shell.execute_reply.started":"2022-06-16T23:14:12.613029Z","shell.execute_reply":"2022-06-16T23:14:12.725402Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData.columns.values","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:15:04.099992Z","iopub.execute_input":"2022-06-16T23:15:04.10041Z","iopub.status.idle":"2022-06-16T23:15:04.106774Z","shell.execute_reply.started":"2022-06-16T23:15:04.10035Z","shell.execute_reply":"2022-06-16T23:15:04.105957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:15:07.02844Z","iopub.execute_input":"2022-06-16T23:15:07.029222Z","iopub.status.idle":"2022-06-16T23:15:07.09616Z","shell.execute_reply.started":"2022-06-16T23:15:07.029187Z","shell.execute_reply":"2022-06-16T23:15:07.095422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# -> Plot of class distributions","metadata":{}},{"cell_type":"code","source":"categories = list(trainData.columns.values[3:])\nsns.set(font_scale = 1)\nplt.figure(figsize=(15,8))\nax= sns.barplot(x = categories, y = trainData.iloc[:,3:].sum().values)\nplt.title(\"Abstract of each category\", fontsize=24)\nplt.ylabel('Number of abstracts', fontsize=18)\nplt.xlabel('Abstract Type ', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = trainData.iloc[:,3:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n    \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:15:09.580254Z","iopub.execute_input":"2022-06-16T23:15:09.581017Z","iopub.status.idle":"2022-06-16T23:15:09.832884Z","shell.execute_reply.started":"2022-06-16T23:15:09.580981Z","shell.execute_reply":"2022-06-16T23:15:09.832141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale = 1)\nplt.figure(figsize=(15,8))\nmultiLabel_counts = trainData.iloc[:,3:].sum(axis=1).value_counts()\n\nax = sns.barplot(x = multiLabel_counts.index,y = multiLabel_counts)\n\nplt.title(\"Abstracts with multiple labels \")\nplt.ylabel('Number of Abstracts', fontsize=18)\nplt.xlabel('Number of labels', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:15:10.604172Z","iopub.execute_input":"2022-06-16T23:15:10.604812Z","iopub.status.idle":"2022-06-16T23:15:10.812954Z","shell.execute_reply.started":"2022-06-16T23:15:10.604774Z","shell.execute_reply":"2022-06-16T23:15:10.812214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Model Implementation From Hugging Face","metadata":{}},{"cell_type":"code","source":"!pip install focal_loss\nfrom focal_loss import BinaryFocalLoss\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, DistilBertTokenizerFast\nimport pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport pickle\n\nprint(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n\n\nclass DistilBertTrain:\n    def __init__(self):\n        self.trainData = pd.read_csv('/kaggle/input/abstract/train.csv', sep=',', header=\"infer\")\n        self.trainData_cp = self.trainData.copy()\n        self.MODEL_NAME = 'distilbert-base-uncased'\n        self.X = 'ABSTRACT'\n        self.Y = [3,4,5,6,7,8]\n        self.num_classes = len(self.Y)\n        self.BATCH_SIZE = 16\n        self.MAX_LENGTH = 512\n        self.N_EPOCHS = 5\n        self.lr=1e-5\n        self.tokenizer = DistilBertTokenizerFast.from_pretrained(self.MODEL_NAME)\n        self.model = TFDistilBertForSequenceClassification.from_pretrained(self.MODEL_NAME, num_labels = self.num_classes)\n        \n    def preprocess(self):\n        self.trainData_cp[self.X] = self.trainData[self.X].apply(lambda x: re.sub(\"\\n\",\" \",x))\n        train_data, val_data = train_test_split(self.trainData_cp, train_size=0.85, random_state=34)\n        val_data, test_data = train_test_split(val_data, train_size=0.7, random_state=34)\n        return train_data, val_data, test_data\n        \n    def distilBertTokenization(self, train_data, val_data, test_data):\n        train_encodings = self.tokenizer(train_data.ABSTRACT.to_list(), truncation=True, padding=True)\n        val_encodings = self.tokenizer(val_data.ABSTRACT.to_list(), truncation=True, padding=True)\n        test_encodings = self.tokenizer(test_data.ABSTRACT.to_list(), truncation=True, padding=True)\n        return train_encodings, val_encodings, test_encodings\n    \n    def distilBertPipelineGeneration(self, train_encodings, val_encodings, train_data, val_data):\n        train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n                                    list(train_data.iloc[:,3:9].values)))\n        val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings),\n                                    list(val_data.iloc[:,3:9].values)))\n        # train\n        tr_pipe = (train_dataset.shuffle(len(train_data.ABSTRACT))\n                  .batch(self.BATCH_SIZE, drop_remainder=True)\n                    .prefetch(tf.data.experimental.AUTOTUNE)\n                      )\n\n  # valid\n        val_pipe = (val_dataset.batch(self.BATCH_SIZE, drop_remainder=True)\n                  .prefetch(tf.data.experimental.AUTOTUNE)\n                    )\n    \n        return tr_pipe, val_pipe\n    \n    def fit(self, tr_data, vl_data):\n        def scheduler(epoch, lr):\n            if epoch < 2:\n                return lr\n            else:\n                return lr * tf.math.exp(-0.1)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate= self.lr)\n\n        earlystp = tf.keras.callbacks.EarlyStopping(\n                    monitor='val_loss',\n                    min_delta=0,\n                    patience=3,\n                    verbose=1,\n                    mode='auto',\n                    baseline=None,\n                    restore_best_weights=True)\n\n\n\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n        loss = BinaryFocalLoss(gamma=2)\n\n        self.model.compile(optimizer, loss, metrics=['accuracy'])\n        self.model.fit(tr_data, epochs=self.N_EPOCHS, batch_size=self.BATCH_SIZE, validation_data = vl_data, callbacks =[lr_schedule, earlystp], verbose=1)\n        return self.model\n    \n    def save(self):\n        model_name = 'distilbert_base_uncased_model'\n        self.model.save_pretrained('./model/+{model_name}+')\n        with open('./model/info.pkl', 'wb') as f:\n            pickle.dump(('distilbert_base_uncased_model', self.MAX_LENGTH), f)\n            \n#     def load(self):\n#         new_model = TFDistilBertForSequenceClassification.from_pretrained('./model/distilbert_base_uncased_model')\n#         self.model_name, self.MAX_LENGTH = pickle.load(open('./model/info.pkl', 'rb'))\n            \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:09:46.970886Z","iopub.execute_input":"2022-06-16T22:09:46.971296Z","iopub.status.idle":"2022-06-16T22:10:23.260371Z","shell.execute_reply.started":"2022-06-16T22:09:46.971215Z","shell.execute_reply":"2022-06-16T22:10:23.259126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Class instance creation and invokation","metadata":{}},{"cell_type":"code","source":"model = DistilBertTrain()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:10:57.921911Z","iopub.execute_input":"2022-06-16T22:10:57.922821Z","iopub.status.idle":"2022-06-16T22:11:22.079608Z","shell.execute_reply.started":"2022-06-16T22:10:57.922785Z","shell.execute_reply":"2022-06-16T22:11:22.078824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"train_data, val_data, test_data = model.preprocess()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:11:22.084119Z","iopub.execute_input":"2022-06-16T22:11:22.086231Z","iopub.status.idle":"2022-06-16T22:11:22.228329Z","shell.execute_reply.started":"2022-06-16T22:11:22.086191Z","shell.execute_reply":"2022-06-16T22:11:22.227412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DistilBERT Tokenization","metadata":{}},{"cell_type":"code","source":"train_encodings, val_encodings, test_encodings = model.distilBertTokenization(train_data, val_data, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:11:22.229602Z","iopub.execute_input":"2022-06-16T22:11:22.230071Z","iopub.status.idle":"2022-06-16T22:11:36.081032Z","shell.execute_reply.started":"2022-06-16T22:11:22.230029Z","shell.execute_reply":"2022-06-16T22:11:36.080209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tenforflow IO Pipeline creation using tf.data ","metadata":{}},{"cell_type":"code","source":"tr_pipe, val_pipe = model.distilBertPipelineGeneration(train_encodings, val_encodings, train_data, val_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:11:36.082908Z","iopub.execute_input":"2022-06-16T22:11:36.083278Z","iopub.status.idle":"2022-06-16T22:12:36.464672Z","shell.execute_reply.started":"2022-06-16T22:11:36.083242Z","shell.execute_reply":"2022-06-16T22:12:36.463789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"distilBert=model.fit(tr_pipe, val_pipe)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:34:59.980842Z","iopub.execute_input":"2022-06-16T23:34:59.981203Z","iopub.status.idle":"2022-06-16T23:34:59.984879Z","shell.execute_reply.started":"2022-06-16T23:34:59.98116Z","shell.execute_reply":"2022-06-16T23:34:59.984124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n  plt.show()\n    \nplot_graphs(distilBert, 'loss')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:35:09.542608Z","iopub.execute_input":"2022-06-16T23:35:09.542975Z","iopub.status.idle":"2022-06-16T23:35:09.54737Z","shell.execute_reply.started":"2022-06-16T23:35:09.542945Z","shell.execute_reply":"2022-06-16T23:35:09.54624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Prediction","metadata":{}},{"cell_type":"code","source":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\ntest_encodings = tokenizer(test_data.ABSTRACT.to_list(), truncation=True, padding=True)\ntest_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n                                    list(test_data.iloc[:,3:9].values)))\ntest_da = (test_dataset.batch(1))\npreds=distilBert.predict(test_da)\npred_labels = [1 if pred >0.5 else 0 for predictions in preds['logits'] for pred in predictions]\ndef logits_to_labels(preds):\n    labels = np.zeros(preds['logits'].shape)\n    for i in np.arange(len(preds['logits'])):\n        for j in np.arange(6):\n            if preds['logits'][i][j] >0.5:\n                labels[i][j] = 1\n            else:\n                continue\n    return labels\n        \nlabels = logits_to_labels(preds)\npredDF =pd.DataFrame(labels, columns = ['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance'])\npredDF.to_csv('predictions.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:50:56.084786Z","iopub.execute_input":"2022-06-17T00:50:56.085188Z","iopub.status.idle":"2022-06-17T00:50:56.196043Z","shell.execute_reply.started":"2022-06-17T00:50:56.085143Z","shell.execute_reply":"2022-06-17T00:50:56.194942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{"execution":{"iopub.status.busy":"2022-06-17T01:02:53.995193Z","iopub.execute_input":"2022-06-17T01:02:53.996069Z","iopub.status.idle":"2022-06-17T01:02:54.016969Z","shell.execute_reply.started":"2022-06-17T01:02:53.996018Z","shell.execute_reply":"2022-06-17T01:02:54.016152Z"}}},{"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix, precision_recall_fscore_support, accuracy_score\nmcm = multilabel_confusion_matrix(test_data.iloc[:,3:].values, labels)\nprint('Multilabel_confusion_matrix \\n', mcm)\nprint('Accuracy -', accuracy_score(test_data.iloc[:,3:].values, labels))","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:53:19.9177Z","iopub.execute_input":"2022-06-17T00:53:19.918069Z","iopub.status.idle":"2022-06-17T00:53:19.923671Z","shell.execute_reply.started":"2022-06-17T00:53:19.91804Z","shell.execute_reply":"2022-06-17T00:53:19.922866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"precision, recall, fscore ,_= precision_recall_fscore_support(test_data.iloc[:,3:].values, labels)\nprint('precision', precision)\nprint('Recall', recall)\nprint('F1 score', fscore)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T01:05:58.059197Z","iopub.execute_input":"2022-06-17T01:05:58.059776Z","iopub.status.idle":"2022-06-17T01:05:58.068701Z","shell.execute_reply.started":"2022-06-17T01:05:58.059732Z","shell.execute_reply":"2022-06-17T01:05:58.067421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"# model_name = 'distilbert_base_uncased_model'\ndistilBert.save_pretrained('./model/distilbert_base_uncased_model')\nwith open('./model/info.pkl', 'wb') as f:\n    pickle.dump(('distilbert_base_uncased_model', 512), f)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:07:47.715407Z","iopub.execute_input":"2022-06-16T23:07:47.715861Z","iopub.status.idle":"2022-06-16T23:07:48.342252Z","shell.execute_reply.started":"2022-06-16T23:07:47.715824Z","shell.execute_reply":"2022-06-16T23:07:48.341307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DistilBERT Model Sentence Prediction","metadata":{}},{"cell_type":"code","source":"import spacy\n# ! python -m spacy download en_core_web_lg\n# spacy_lg = spacy.load('en_core_web_lg')\n! python -m spacy download en_core_web_sm\nspacy_sm = spacy.load('en_core_web_sm')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentenceLevelPrediction(text, model):\n    sent_list = []\n    for sent in spacy_sm(text).sents:\n        sent_list.append(str(list(sent)))\n\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n    test_sent_encodings = tokenizer(sent_list, truncation=True, padding=True)\n    sent_test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_sent_encodings),\n                                    list(np.zeros([len(sent_list),6]))))\n    test_da = (sent_test_dataset.batch(1))\n    sent_preds = model.predict(test_da)\n    return sent_preds\nsent_test_preds = sentenceLevelPrediction(test_data.ABSTRACT.values[3], distilbert_model)\n\nsent_labels = logits_to_labels(sent_test_preds)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Sentence level prediction - \\n', sent_labels)\nprint('Text Label -', test_data.iloc[2,3:].values)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dynamic sentence splitting and check for the confidence on each consecutive addition to last sentence","metadata":{}},{"cell_type":"code","source":"import spacy\n# ! python -m spacy download en_core_web_lg\n# spacy_lg = spacy.load('en_core_web_lg')\n! python -m spacy download en_core_web_sm\nspacy_sm = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:50:08.010841Z","iopub.execute_input":"2022-06-16T10:50:08.011301Z","iopub.status.idle":"2022-06-16T10:50:27.767099Z","shell.execute_reply.started":"2022-06-16T10:50:08.011265Z","shell.execute_reply":"2022-06-16T10:50:27.765866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef sentenceSegmentation(text):\n    sent_list = list(spacy_sm(train_data.ABSTRACT[0]).sents)\n    return sent_list\n\ndef sent_predict(sent_list, model, actual_pred):\n    n = len(sent_list)\n    pred_list = []\n    actual_list = []\n    pred_list = [model.predict(sent) for sent in sent_list]\n    actual_list = n * actual_pred\n    f1_score(actual_list, pred_list,average=\"samples\", )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef dynamicSentenceSelection(text, model):\n    sent_list = list(spacy_sm(train_data.ABSTRACT[0]).sents)\n    num_sents = len(sent_list)\n    prob_list = np.zeros([num_sents])\n    \n    for i in np.arange(num_sents):\n        np.append(prob_list, model.predict(sent_list[:i]))\n        \n    np.mean(prob_list)\n    return prob_list\n        \ndynamicSentenceSelection(train_data.ABSTRACT[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:51:32.573019Z","iopub.execute_input":"2022-06-16T10:51:32.573428Z","iopub.status.idle":"2022-06-16T10:51:32.708001Z","shell.execute_reply.started":"2022-06-16T10:51:32.573383Z","shell.execute_reply":"2022-06-16T10:51:32.707222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Hyperparameter Tuning\n\n1. Learning rate\n2. Early stopping\n3. Gradient clipping\n4. number of epochs to train\n5. dropout\n6. regularization","metadata":{},"execution_count":null,"outputs":[]}]}