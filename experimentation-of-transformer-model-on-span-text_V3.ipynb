{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-06-15T22:01:04.340628Z","iopub.execute_input":"2022-06-15T22:01:04.341203Z","iopub.status.idle":"2022-06-15T22:01:05.221211Z","shell.execute_reply.started":"2022-06-15T22:01:04.34109Z","shell.execute_reply":"2022-06-15T22:01:05.219988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# with tpu_strategy.scope():\n#     model = tf.keras.Sequential( … ) # define your model normally\n#     model.compile( … )\n    \n# model.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=…)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/abstract'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        print(dirname)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-16T23:13:47.752178Z","iopub.execute_input":"2022-06-16T23:13:47.752545Z","iopub.status.idle":"2022-06-16T23:13:47.78009Z","shell.execute_reply.started":"2022-06-16T23:13:47.752514Z","shell.execute_reply":"2022-06-16T23:13:47.779237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrainData = pd.read_csv('/kaggle/input/abstract/train.csv', sep=',', header=\"infer\")\ntestData = pd.read_csv('/kaggle/input/abstract/test.csv', sep = ',', header = \"infer\")\ntestLabels = pd.read_csv('/kaggle/input/abstract/sample_submission.csv', sep = ',', header = \"infer\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:48.940171Z","iopub.execute_input":"2022-06-16T23:13:48.940548Z","iopub.status.idle":"2022-06-16T23:13:49.377509Z","shell.execute_reply.started":"2022-06-16T23:13:48.940518Z","shell.execute_reply":"2022-06-16T23:13:49.376731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, remaining = train_test_split(trainData, train_size=0.85, random_state=34)\ntest_data, val_data = train_test_split(remaining, train_size=0.7, random_state=34)\ntrain_data.shape, val_data.shape, test_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:51.072184Z","iopub.execute_input":"2022-06-16T23:13:51.072603Z","iopub.status.idle":"2022-06-16T23:13:51.088922Z","shell.execute_reply.started":"2022-06-16T23:13:51.072562Z","shell.execute_reply":"2022-06-16T23:13:51.087541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Exploration","metadata":{}},{"cell_type":"code","source":"trainData.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:54.198518Z","iopub.execute_input":"2022-06-16T23:13:54.19902Z","iopub.status.idle":"2022-06-16T23:13:54.219103Z","shell.execute_reply.started":"2022-06-16T23:13:54.198988Z","shell.execute_reply":"2022-06-16T23:13:54.218424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:55.358278Z","iopub.execute_input":"2022-06-16T23:13:55.358933Z","iopub.status.idle":"2022-06-16T23:13:55.397934Z","shell.execute_reply.started":"2022-06-16T23:13:55.358896Z","shell.execute_reply":"2022-06-16T23:13:55.397193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainData[trainData['Computer Science'] == 1])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:13:56.573937Z","iopub.execute_input":"2022-06-16T23:13:56.574285Z","iopub.status.idle":"2022-06-16T23:13:56.582204Z","shell.execute_reply.started":"2022-06-16T23:13:56.574256Z","shell.execute_reply":"2022-06-16T23:13:56.581345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(trainData.groupby(['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']).aggregate('Quantitative Finance'))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:14:12.612477Z","iopub.execute_input":"2022-06-16T23:14:12.613072Z","iopub.status.idle":"2022-06-16T23:14:12.726468Z","shell.execute_reply.started":"2022-06-16T23:14:12.613029Z","shell.execute_reply":"2022-06-16T23:14:12.725402Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainData.columns.values","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:15:04.099992Z","iopub.execute_input":"2022-06-16T23:15:04.10041Z","iopub.status.idle":"2022-06-16T23:15:04.106774Z","shell.execute_reply.started":"2022-06-16T23:15:04.10035Z","shell.execute_reply":"2022-06-16T23:15:04.105957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:15:07.02844Z","iopub.execute_input":"2022-06-16T23:15:07.029222Z","iopub.status.idle":"2022-06-16T23:15:07.09616Z","shell.execute_reply.started":"2022-06-16T23:15:07.029187Z","shell.execute_reply":"2022-06-16T23:15:07.095422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# -> Plot of class distributions","metadata":{}},{"cell_type":"code","source":"categories = list(trainData.columns.values[3:])\nsns.set(font_scale = 1)\nplt.figure(figsize=(15,8))\nax= sns.barplot(x = categories, y = trainData.iloc[:,3:].sum().values)\nplt.title(\"Abstract of each category\", fontsize=24)\nplt.ylabel('Number of abstracts', fontsize=18)\nplt.xlabel('Abstract Type ', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = trainData.iloc[:,3:].sum().values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n    \nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:15:09.580254Z","iopub.execute_input":"2022-06-16T23:15:09.581017Z","iopub.status.idle":"2022-06-16T23:15:09.832884Z","shell.execute_reply.started":"2022-06-16T23:15:09.580981Z","shell.execute_reply":"2022-06-16T23:15:09.832141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale = 1)\nplt.figure(figsize=(15,8))\nmultiLabel_counts = trainData.iloc[:,3:].sum(axis=1).value_counts()\n\nax = sns.barplot(x = multiLabel_counts.index,y = multiLabel_counts)\n\nplt.title(\"Abstracts with multiple labels \")\nplt.ylabel('Number of Abstracts', fontsize=18)\nplt.xlabel('Number of labels', fontsize=18)\n#adding the text labels\nrects = ax.patches\nlabels = multiLabel_counts.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:15:10.604172Z","iopub.execute_input":"2022-06-16T23:15:10.604812Z","iopub.status.idle":"2022-06-16T23:15:10.812954Z","shell.execute_reply.started":"2022-06-16T23:15:10.604774Z","shell.execute_reply":"2022-06-16T23:15:10.812214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Model Implementation From Hugging Face","metadata":{}},{"cell_type":"code","source":"!pip install focal_loss\nfrom focal_loss import BinaryFocalLoss\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, DistilBertTokenizerFast\nimport pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport pickle\n\nprint(\"TF Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n\n\nclass DistilBertTrain:\n    def __init__(self):\n        self.trainData = pd.read_csv('/kaggle/input/abstract/train.csv', sep=',', header=\"infer\")\n        self.trainData_cp = self.trainData.copy()\n        self.MODEL_NAME = 'distilbert-base-uncased'\n        self.X = 'ABSTRACT'\n        self.Y = [3,4,5,6,7,8]\n        self.num_classes = len(self.Y)\n        self.BATCH_SIZE = 16\n        self.MAX_LENGTH = 512\n        self.N_EPOCHS = 5\n        self.lr=1e-5\n        self.tokenizer = DistilBertTokenizerFast.from_pretrained(self.MODEL_NAME)\n        self.model = TFDistilBertForSequenceClassification.from_pretrained(self.MODEL_NAME, num_labels = self.num_classes)\n        \n    def preprocess(self):\n        self.trainData_cp[self.X] = self.trainData[self.X].apply(lambda x: re.sub(\"\\n\",\" \",x))\n        train_data, val_data = train_test_split(self.trainData_cp, train_size=0.85, random_state=34)\n        val_data, test_data = train_test_split(val_data, train_size=0.7, random_state=34)\n        return train_data, val_data, test_data\n        \n    def distilBertTokenization(self, train_data, val_data, test_data):\n        train_encodings = self.tokenizer(train_data.ABSTRACT.to_list(), truncation=True, padding=True)\n        val_encodings = self.tokenizer(val_data.ABSTRACT.to_list(), truncation=True, padding=True)\n        test_encodings = self.tokenizer(test_data.ABSTRACT.to_list(), truncation=True, padding=True)\n        return train_encodings, val_encodings, test_encodings\n    \n    def distilBertPipelineGeneration(self, train_encodings, val_encodings, train_data, val_data):\n        train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),\n                                    list(train_data.iloc[:,3:9].values)))\n        val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings),\n                                    list(val_data.iloc[:,3:9].values)))\n        # train\n        tr_pipe = (train_dataset.shuffle(len(train_data.ABSTRACT))\n                  .batch(self.BATCH_SIZE, drop_remainder=True)\n                    .prefetch(tf.data.experimental.AUTOTUNE)\n                      )\n\n  # valid\n        val_pipe = (val_dataset.batch(self.BATCH_SIZE, drop_remainder=True)\n                  .prefetch(tf.data.experimental.AUTOTUNE)\n                    )\n    \n        return tr_pipe, val_pipe\n    \n    def fit(self, tr_data, vl_data):\n        def scheduler(epoch, lr):\n            if epoch < 2:\n                return lr\n            else:\n                return lr * tf.math.exp(-0.1)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate= self.lr)\n\n        earlystp = tf.keras.callbacks.EarlyStopping(\n                    monitor='val_loss',\n                    min_delta=0,\n                    patience=3,\n                    verbose=1,\n                    mode='auto',\n                    baseline=None,\n                    restore_best_weights=True)\n\n\n\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n        loss = BinaryFocalLoss(gamma=2)\n\n        self.model.compile(optimizer, loss, metrics=['accuracy'])\n        self.model.fit(tr_data, epochs=self.N_EPOCHS, batch_size=self.BATCH_SIZE, validation_data = vl_data, callbacks =[lr_schedule, earlystp], verbose=1)\n        return self.model\n    \n    def save(self):\n        model_name = 'distilbert_base_uncased_model'\n        self.model.save_pretrained('./model/+{model_name}+')\n        with open('./model/info.pkl', 'wb') as f:\n            pickle.dump(('distilbert_base_uncased_model', self.MAX_LENGTH), f)\n            \n#     def load(self):\n#         new_model = TFDistilBertForSequenceClassification.from_pretrained('./model/distilbert_base_uncased_model')\n#         self.model_name, self.MAX_LENGTH = pickle.load(open('./model/info.pkl', 'rb'))\n            \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:09:46.970886Z","iopub.execute_input":"2022-06-16T22:09:46.971296Z","iopub.status.idle":"2022-06-16T22:10:23.260371Z","shell.execute_reply.started":"2022-06-16T22:09:46.971215Z","shell.execute_reply":"2022-06-16T22:10:23.259126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Class instance creation and invokation","metadata":{}},{"cell_type":"code","source":"model = DistilBertTrain()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:10:57.921911Z","iopub.execute_input":"2022-06-16T22:10:57.922821Z","iopub.status.idle":"2022-06-16T22:11:22.079608Z","shell.execute_reply.started":"2022-06-16T22:10:57.922785Z","shell.execute_reply":"2022-06-16T22:11:22.078824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"train_data, val_data, test_data = model.preprocess()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:11:22.084119Z","iopub.execute_input":"2022-06-16T22:11:22.086231Z","iopub.status.idle":"2022-06-16T22:11:22.228329Z","shell.execute_reply.started":"2022-06-16T22:11:22.086191Z","shell.execute_reply":"2022-06-16T22:11:22.227412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DistilBERT Tokenization","metadata":{}},{"cell_type":"code","source":"train_encodings, val_encodings, test_encodings = model.distilBertTokenization(train_data, val_data, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:11:22.229602Z","iopub.execute_input":"2022-06-16T22:11:22.230071Z","iopub.status.idle":"2022-06-16T22:11:36.081032Z","shell.execute_reply.started":"2022-06-16T22:11:22.230029Z","shell.execute_reply":"2022-06-16T22:11:36.080209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tenforflow IO Pipeline creation using tf.data ","metadata":{}},{"cell_type":"code","source":"tr_pipe, val_pipe = model.distilBertPipelineGeneration(train_encodings, val_encodings, train_data, val_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T22:11:36.082908Z","iopub.execute_input":"2022-06-16T22:11:36.083278Z","iopub.status.idle":"2022-06-16T22:12:36.464672Z","shell.execute_reply.started":"2022-06-16T22:11:36.083242Z","shell.execute_reply":"2022-06-16T22:12:36.463789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"distilBert=model.fit(tr_pipe, val_pipe)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:34:59.980842Z","iopub.execute_input":"2022-06-16T23:34:59.981203Z","iopub.status.idle":"2022-06-16T23:34:59.984879Z","shell.execute_reply.started":"2022-06-16T23:34:59.981160Z","shell.execute_reply":"2022-06-16T23:34:59.984124Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n  plt.show()\n    \nplot_graphs(distilBert, 'loss')\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:35:09.542608Z","iopub.execute_input":"2022-06-16T23:35:09.542975Z","iopub.status.idle":"2022-06-16T23:35:09.547370Z","shell.execute_reply.started":"2022-06-16T23:35:09.542945Z","shell.execute_reply":"2022-06-16T23:35:09.546240Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"test_data.ABSTRACT[:3].to_list()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:50:05.939400Z","iopub.execute_input":"2022-06-16T23:50:05.941703Z","iopub.status.idle":"2022-06-16T23:50:05.951151Z","shell.execute_reply.started":"2022-06-16T23:50:05.941662Z","shell.execute_reply":"2022-06-16T23:50:05.950149Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"['  We present a new proof of a fundamental result concerning cycles of random\\npermutations which gives some intuition for the connection between Touchard\\npolynomials and the Poisson distribution. We also introduce a rather novel\\npermutation statistic and study its distribution. This quantity, indexed by\\n$m$, is the number of sets of size $m$ fixed by the permutation. This leads to\\na new and simpler derivation of the exponential generating function for the\\nnumber of covers of certain multisets.\\n',\n '  This paper is a contribution to the study of the universal Horn fragment of\\npredicate fuzzy logics, focusing on the proof of the existence of free models\\nof theories of Horn clauses over Rational Pavelka predicate logic. We define\\nthe notion of a term structure associated to every consistent theory T over\\nRational Pavelka predicate logic and we prove that the term models of T are\\nfree on the class of all models of T. Finally, it is shown that if T is a set\\nof Horn clauses, the term structure associated to T is a model of T.\\n',\n '  Abridged: We used the fourth internal data release of the Gaia-ESO survey to\\ncharacterize the bulge chemistry, spatial distribution, kinematics, and to\\ncompare it chemically with the thin and thick disks. The sample consist on\\n~2500 red clump stars in 11 bulge fields ($-10^\\\\circ\\\\leq l\\\\leq+8^\\\\circ$ and\\n$-10^\\\\circ\\\\leq b\\\\leq-4^\\\\circ$), and a set of ~6300 disk stars selected for\\ncomparison. The bulge MDF is confirmed to be bimodal across the whole sampled\\narea, with metal-poor stars dominating at high latitudes. The metal-rich stars\\nexhibit bar-like kinematics and display a bimodality in their magnitude\\ndistribution, a feature which is tightly associated with the X-shape bulge.\\nThey overlap with the metal-rich end of the thin disk sequence in the [Mg/Fe]\\nvs. [Fe/H] plane. Metal-poor bulge stars have a more isotropic hot kinematics\\nand do not participate in the X-shape bulge. With similar Mg-enhancement\\nlevels, the position of the metal-poor bulge sequence \"knee\" is observed at\\n[Fe/H]$_{knee}=-0.37\\\\pm0.09$, being 0.06 dex higher than that of the thick\\ndisk. It suggests a higher SFR for the bulge than for the thick disk. Finally,\\nwe present a chemical evolution model that suitably fits the whole bulge\\nsequence by assuming a fast ($<1$ Gyr) intense burst of stellar formation at\\nearly epochs. We associate metal-rich stars with the B/P bulge formed from the\\nsecular evolution of the early thin disk. On the other hand, the metal-poor\\nsubpopulation might be the product of an early prompt dissipative collapse\\ndominated by massive stars. Nevertheless, our results do not allow us to firmly\\nrule out the possibility that these stars come from the secular evolution of\\nthe early thick disk. This is the first time that an analysis of the bulge MDF\\nand $\\\\alpha$-abundances has been performed in a large area on the basis of a\\nhomogeneous, fully spectroscopic analysis of high-resolution, high S/N data.\\n']"},"metadata":{}}]},{"cell_type":"code","source":"# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n# test_encodings = tokenizer(test_data.ABSTRACT.to_list(), truncation=True, padding=True)\n# test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),\n#                                     list(test_data.iloc[:,3:9].values)))\n# test_da = (test_dataset.batch(1))\n# preds=distilBert.predict(test_da)\n# pred_labels = [1 if pred >0.5 else 0 for predictions in preds['logits'] for pred in predictions]\ndef logits_to_labels(preds):\n    labels = np.zeros(preds['logits'].shape)\n    for i in np.arange(len(preds['logits'])):\n        for j in np.arange(6):\n            if preds['logits'][i][j] >0.5:\n                labels[i][j] = 1\n            else:\n                continue\n    return labels\n        \nlabels = logits_to_labels(preds)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:50:56.084786Z","iopub.execute_input":"2022-06-17T00:50:56.085188Z","iopub.status.idle":"2022-06-17T00:50:56.196043Z","shell.execute_reply.started":"2022-06-17T00:50:56.085143Z","shell.execute_reply":"2022-06-17T00:50:56.194942Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"predDF =pd.DataFrame(labels, columns = ['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance'])\npredDF.to_csv('predictions.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T01:02:53.995193Z","iopub.execute_input":"2022-06-17T01:02:53.996069Z","iopub.status.idle":"2022-06-17T01:02:54.016969Z","shell.execute_reply.started":"2022-06-17T01:02:53.996018Z","shell.execute_reply":"2022-06-17T01:02:54.016152Z"},"trusted":true},"execution_count":185,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\ncm = confusion_matrix(test_data.iloc[:,3:].values.argmax(axis=1), labels.argmax(axis=1))\ncm","metadata":{"execution":{"iopub.status.busy":"2022-06-17T00:53:19.917700Z","iopub.execute_input":"2022-06-17T00:53:19.918069Z","iopub.status.idle":"2022-06-17T00:53:19.923671Z","shell.execute_reply.started":"2022-06-17T00:53:19.918040Z","shell.execute_reply":"2022-06-17T00:53:19.922866Z"},"trusted":true},"execution_count":172,"outputs":[{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"array([[831,  10,  36,  24,   2,   0],\n       [ 34, 519,  20,   2,   6,   0],\n       [ 45,  22, 380,  24,   0,   0],\n       [121,   4,  14,  41,   5,   0],\n       [ 16,   9,   0,   2,  16,   0],\n       [ 17,   1,   1,   0,   0,   0]])"},"metadata":{}}]},{"cell_type":"code","source":"# precision, recall, fscore ,_= precision_recall_fscore_support(test_data.iloc[:,3:].values, labels)\nprint('precision', precision)\nprint('Recall', recall)\nprint('F1 score', fscore)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T01:05:58.059197Z","iopub.execute_input":"2022-06-17T01:05:58.059776Z","iopub.status.idle":"2022-06-17T01:05:58.068701Z","shell.execute_reply.started":"2022-06-17T01:05:58.059732Z","shell.execute_reply":"2022-06-17T01:05:58.067421Z"},"trusted":true},"execution_count":191,"outputs":[{"name":"stdout","text":"precision [0.80765456 0.91840278 0.86705202 0.79044118 0.63636364 0.        ]\nRecall [0.91140642 0.84775641 0.73529412 0.78181818 0.36842105 0.        ]\nF1 score [0.85639958 0.88166667 0.79575597 0.78610603 0.46666667 0.        ]\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_name = 'distilbert_base_uncased_model'\ndistilBert.save_pretrained('./model/distilbert_base_uncased_model')\nwith open('./model/info.pkl', 'wb') as f:\n    pickle.dump(('distilbert_base_uncased_model', 512), f)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T23:07:47.715407Z","iopub.execute_input":"2022-06-16T23:07:47.715861Z","iopub.status.idle":"2022-06-16T23:07:48.342252Z","shell.execute_reply.started":"2022-06-16T23:07:47.715824Z","shell.execute_reply":"2022-06-16T23:07:48.341307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Prediction","metadata":{}},{"cell_type":"code","source":"class DistilBertPredict:\n    def __init__(self, model, test_data):\n        self.\n    \n    \n#     def predict(self, test_data):","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DistilBertConfig()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T11:19:06.330527Z","iopub.execute_input":"2022-06-16T11:19:06.330914Z","iopub.status.idle":"2022-06-16T11:19:06.338123Z","shell.execute_reply.started":"2022-06-16T11:19:06.330882Z","shell.execute_reply":"2022-06-16T11:19:06.33728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dynamic sentence splitting and check for the confidence on each consecutive addition to last sentence","metadata":{}},{"cell_type":"code","source":"import spacy\n# ! python -m spacy download en_core_web_lg\n# spacy_lg = spacy.load('en_core_web_lg')\n! python -m spacy download en_core_web_sm\nspacy_sm = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:50:08.010841Z","iopub.execute_input":"2022-06-16T10:50:08.011301Z","iopub.status.idle":"2022-06-16T10:50:27.767099Z","shell.execute_reply.started":"2022-06-16T10:50:08.011265Z","shell.execute_reply":"2022-06-16T10:50:27.765866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef dynamicSentenceSelection(text, model):\n    sent_list = list(spacy_sm(train_data.ABSTRACT[0]).sents)\n    num_sents = len(sent_list)\n    prob_list = np.zeros([num_sents])\n    \n    for i in np.arange(num_sents):\n        np.append(prob_list, model.predict(sent_list[:i]))\n        \n    np.mean(prob_list)\n    return prob_list\n        \ndynamicSentenceSelection(train_data.ABSTRACT[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T10:51:32.573019Z","iopub.execute_input":"2022-06-16T10:51:32.573428Z","iopub.status.idle":"2022-06-16T10:51:32.708001Z","shell.execute_reply.started":"2022-06-16T10:51:32.573383Z","shell.execute_reply":"2022-06-16T10:51:32.707222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Hyperparameter Tuning\n\n1. Learning rate\n2. Early stopping\n3. Gradient clipping\n4. number of epochs to train\n5. dropout\n6. regularization","metadata":{},"execution_count":null,"outputs":[]}]}